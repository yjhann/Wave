import os
import json
import glob
from datetime import datetime
from typing import Dict, Any, List

import numpy as np
import torch
from diffusers import AudioLDMPipeline
from scipy.io.wavfile import write as wav_write

from audio_prompt import generate_prompts
from utils import ensure_dir, sanitize_filename


def _load_pipeline(model_id: str, hf_token: str | None) -> AudioLDMPipeline:
    """AudioLDM2 íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    
    print(f"AudioLDM2 ëª¨ë¸ ë¡œë”© ì¤‘... (Device: {device}, Dtype: {dtype})")
    
    pipe = AudioLDMPipeline.from_pretrained(
        model_id,
        torch_dtype=dtype,
        use_auth_token=hf_token,
    )
    pipe = pipe.to(device)
    
    print(f"âœ… AudioLDM2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! Device: {pipe.device}")
    return pipe


# _ensure_dir í•¨ìˆ˜ëŠ” utils.pyì˜ ensure_dirë¡œ ëŒ€ì²´ë¨


def _load_json(path: str) -> Dict[str, Any]:
    """JSON íŒŒì¼ ë¡œë“œ"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _objects_to_sound_sources_if_needed(data: Dict[str, Any]) -> Dict[str, Any]:
    """objectsë¥¼ sound_sourcesë¡œ ë§¤í•‘ (í˜¸í™˜ì„±)"""
    if "sound_sources" in data:
        return data
    if "objects" in data and isinstance(data["objects"], list):
        return {
            **data,
            "sound_sources": data["objects"],
        }
    return data


# _sanitize_filename í•¨ìˆ˜ëŠ” utils.pyì˜ sanitize_filenameìœ¼ë¡œ ëŒ€ì²´ë¨


def _save_wav(path: str, audio: np.ndarray, sample_rate: int = 16000) -> None:
    """ì˜¤ë””ì˜¤ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥"""
    # Ensure mono float32 in -1..1 -> int16
    if audio.ndim > 1:
        audio = np.mean(audio, axis=0)
    audio = np.clip(audio, -1.0, 1.0)
    wav = (audio * 32767.0).astype(np.int16)
    wav_write(path, sample_rate, wav)


def generate_audio_for_sound_sources(
    sound_source_dir: str = "sound_sources",
    result_dir: str = "result",
    model_id: str = "cvssp/audioldm-s-full-v2",
    audio_seconds: float = 4.0,
    steps: int = 200,
    guidance: float = 3.5,
    seed: int | None = None,
    single: str | None = None,
) -> None:
    """Sound sources JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë””ì˜¤ ìƒì„±"""
    
    ensure_dir(result_dir)

    hf_token = os.environ.get("HUGGING_FACE_TOKEN") or os.environ.get("HF_TOKEN")
    pipe = _load_pipeline(model_id, hf_token)

    if seed is not None:
        generator = torch.Generator(device=pipe.device).manual_seed(seed)
    else:
        generator = torch.Generator(device=pipe.device)

    # JSON íŒŒì¼ë“¤ ì°¾ê¸°
    if single:
        if single.lower().endswith(".json"):
            candidate = single
        else:
            # ì´ë¯¸ì§€ íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: data/101.jpg -> 101)
            base_name = os.path.splitext(os.path.basename(single))[0]
            # sound_sources ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ ì´ë¯¸ì§€ì˜ JSON íŒŒì¼ ì°¾ê¸°
            json_pattern = os.path.join(sound_source_dir, base_name, f"{base_name}_sound_source.json")
            candidate = json_pattern if os.path.exists(json_pattern) else single
        json_files = [candidate] if os.path.exists(candidate) else []
    else:
        # sound_sources ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì´ë¯¸ì§€ í´ë”ì—ì„œ JSON íŒŒì¼ ì°¾ê¸°
        json_files = []
        for image_folder in os.listdir(sound_source_dir):
            image_folder_path = os.path.join(sound_source_dir, image_folder)
            if os.path.isdir(image_folder_path):
                json_pattern = os.path.join(image_folder_path, "*_sound_source.json")
                json_files.extend(glob.glob(json_pattern))
        json_files = sorted(json_files)
    
    if not json_files:
        print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sound_source_dir}")
        return

    print(f"ğŸ“ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬. ì¶œë ¥ -> {result_dir}")

    total_audio_generated = 0
    
    for json_path in json_files:
        print(f"\nğŸµ ì²˜ë¦¬ ì¤‘: {json_path}")
        
        # ì´ë¯¸ì§€ ì´ë¦„ ì¶”ì¶œ (í´ë”ëª… ë˜ëŠ” íŒŒì¼ëª…ì—ì„œ)
        if os.path.dirname(json_path) != sound_source_dir:
            # í•˜ìœ„ í´ë”ì— ìˆëŠ” ê²½ìš°
            base = os.path.basename(os.path.dirname(json_path))
        else:
            # ì§ì ‘ sound_sourcesì— ìˆëŠ” ê²½ìš°
            base = os.path.splitext(os.path.basename(json_path))[0].replace("_sound_source", "")
        
        try:
            data = _objects_to_sound_sources_if_needed(_load_json(json_path))
            prompts: List[Dict[str, Any]] = generate_prompts(data)
            
            if not prompts:
                print(f"  âš ï¸ {json_path}ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                continue

            # ì´ë¯¸ì§€ë³„ ê²°ê³¼ í´ë” ìƒì„±
            image_out_dir = os.path.join(result_dir, base)
            ensure_dir(image_out_dir)

            print(f"  ğŸ¯ {len(prompts)}ê°œ ì˜¤ë””ì˜¤ í´ë¦½ ìƒì„± ì¤‘...")
            
            for idx, item in enumerate(prompts, 1):
                prompt_text = item["prompt"]
                source_name = sanitize_filename(item.get("source_name", "source"))
                play_method = sanitize_filename(str(item.get("play_method", "act")))

                out_name = f"{idx:02d}_{source_name}_{play_method}.wav"
                out_path = os.path.join(image_out_dir, out_name)

                try:
                    with torch.autocast(device_type=("cuda" if torch.cuda.is_available() else "cpu")):
                        audio = pipe(
                            prompt_text,
                            num_inference_steps=steps,
                            audio_length_in_s=audio_seconds,
                            guidance_scale=guidance,
                            generator=generator,
                        ).audios[0]

                    _save_wav(out_path, np.array(audio), sample_rate=16000)
                    print(f"    âœ… {out_name}")
                    total_audio_generated += 1
                    
                except Exception as e:
                    print(f"    âŒ {out_name} ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ë“¤ì„ ì¶”ì ìš©ìœ¼ë¡œ ì €ì¥
            prompts_dump = os.path.join(image_out_dir, "prompts.json")
            with open(prompts_dump, "w", encoding="utf-8") as f:
                json.dump(prompts, f, ensure_ascii=False, indent=2)
            
            print(f"  ğŸ“„ í”„ë¡¬í”„íŠ¸ ì €ì¥: {prompts_dump}")
            
        except Exception as e:
            print(f"  âŒ {json_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    print(f"\nğŸ‰ ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {total_audio_generated}ê°œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {result_dir}")


def run_generation(
    sound_source_dir: str = "sound_sources",
    result_dir: str = "result",
    model_id: str = "cvssp/audioldm-s-full-v2",
    audio_seconds: float = 4.0,
    steps: int = 200,
    guidance: float = 3.5,
    seed: int | None = None,
    single: str | None = None,
) -> None:
    """ì˜¤ë””ì˜¤ ìƒì„± ì‹¤í–‰"""
    try:
        generate_audio_for_sound_sources(
            sound_source_dir=sound_source_dir,
            result_dir=result_dir,
            model_id=model_id,
            audio_seconds=audio_seconds,
            steps=steps,
            guidance=guidance,
            seed=seed,
            single=single,
        )
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="AudioLDM2ë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ìƒì„±")
    parser.add_argument("--src", type=str, default="sound_sources", help="Sound sources ë””ë ‰í† ë¦¬")
    parser.add_argument("--out", type=str, default="result", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model", type=str, default="cvssp/audioldm-s-full-v2", help="AudioLDM ëª¨ë¸ ID")
    parser.add_argument("--seconds", type=float, default=4.0, help="ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)")
    parser.add_argument("--steps", type=int, default=200, help="Diffusion ìŠ¤í… ìˆ˜")
    parser.add_argument("--guidance", type=float, default=3.5, help="Guidance scale")
    parser.add_argument("--seed", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--single", type=str, default=None, help="ë‹¨ì¼ ìƒ˜í”Œ: ì´ë¯¸ì§€ ì´ë¦„ (ì˜ˆ: 101) ë˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()

    print("ğŸµ AudioLDM2 ì˜¤ë””ì˜¤ ìƒì„±ê¸°")
    print(f"ğŸ“ ì†ŒìŠ¤: {args.src}")
    print(f"ğŸ“ ì¶œë ¥: {args.out}")
    print(f"ğŸ¤– ëª¨ë¸: {args.model}")

    run_generation(
        sound_source_dir=args.src,
        result_dir=args.out,
        model_id=args.model,
        audio_seconds=args.seconds,
        steps=args.steps,
        guidance=args.guidance,
        seed=args.seed,
        single=args.single,
    )
